{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9755fa",
   "metadata": {},
   "source": [
    "### 사용한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21597e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "from matplotlib import pyplot as plt \n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0128936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e9568",
   "metadata": {},
   "source": [
    "### 손을 인식해서 좌표를 예측하는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5435824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False \n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results  \n",
    "    #손을 인식해서 좌표를 예측하는 메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8a96b",
   "metadata": {},
   "source": [
    "### 이미지에 점을 그리는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f493d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    #이미지에 점을 그리는 메서드 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e55120",
   "metadata": {},
   "source": [
    "### 카메라 화면 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005a386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened():\n",
    "    \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "    \n",
    "        #draw_landmarks(image,results)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10)&0xFF == ord('a'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #메서드 사용해봤다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82aeca",
   "metadata": {},
   "source": [
    "### keypoint 추출하는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d900b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh]) #키포인트 추출하는 메서드 (예측하기 쉽게 데이터를 정리한거야)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd4108",
   "metadata": {},
   "source": [
    "### 학습데이터 파일경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93d532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_Data4') #학습데이터 저장하려고 파일경로 설정\n",
    "\n",
    "actions = np.array(['yes', 'no', 'hate',\"don't\",\"i'm fine\",'sorry',\"Let's go\",'do it','glad','good','bad','tired','follow me'])\n",
    "\n",
    "no_sequences = 50\n",
    "\n",
    "sequence_length = 30 #한시퀀스에 30프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296f1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        #파일경로를 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7bd2b",
   "metadata": {},
   "source": [
    "### 학습데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860719a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    action = actions[12]\n",
    "    for sequence in range(no_sequences):\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = cap.read()\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw_landmarks(image, results)\n",
    "                \n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collection frames for {} video Number {}'.format(action, sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1,cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(2000)\n",
    "            else:\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1,cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "                \n",
    "            if cv2.waitKey(10) & 0xFF == ord('a'):\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() \n",
    "    #학습데이터 만드는중, 데이터가 배열형식으로 저장됐다... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f88dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1542173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71032c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46571d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 0,\n",
       " 'no': 1,\n",
       " 'hate': 2,\n",
       " \"don't\": 3,\n",
       " \"i'm fine\": 4,\n",
       " 'sorry': 5,\n",
       " \"Let's go\": 6,\n",
       " 'do it': 7,\n",
       " 'glad': 8,\n",
       " 'good': 9,\n",
       " 'bad': 10,\n",
       " 'tired': 11,\n",
       " 'follow me': 12}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map #딕셔너리형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1587c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "        #학습데이터 불러오고 레이블링- 문제의 답을 알려주는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58f00300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 30, 258)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape #인풋(문제)데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d486d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ed5456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 30, 258)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6bca795",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d39e41e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]#Label과 신경망의 출력값을 비교하기 위해서 대응되도록 정수값 하나를 뉴런의 개수에 맞게 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f71bb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e82555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f09571a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58ba160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258))) #1.입력데이터와 입력층의 갯수를 맞춰줘야한다. \n",
    "#lstm 입력, 삭제, 출력게이트를 이용해서 셀상태 은닉상태를 계산한다.불필요한데이터지우고 오래 기억돼야할 데이터 정하는거\n",
    "#rnn 과 비교했을때 긴 시퀀스의 입력을 처리하는데 좋음. \n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))#마지막 레이어의 출력이 레이블이기때문에 (레이블이 모든 경우의수를 나타내기때문에)\n",
    "#2.출력층의 갯수와 레이블의 갯수를 맞춰줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d49cd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91e45226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 6s 132ms/step - loss: 2.6063 - categorical_accuracy: 0.6418\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.7819 - categorical_accuracy: 0.8006\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.5101 - categorical_accuracy: 0.8963\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.4048 - categorical_accuracy: 0.9335\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.3707 - categorical_accuracy: 0.9173\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3349 - categorical_accuracy: 0.9190\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2826 - categorical_accuracy: 0.9222\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.1980 - categorical_accuracy: 0.9433\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.1618 - categorical_accuracy: 0.9514\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.1479 - categorical_accuracy: 0.9498\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 0.1233 - categorical_accuracy: 0.9627\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0989 - categorical_accuracy: 0.9741\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0915 - categorical_accuracy: 0.9806\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.1772 - categorical_accuracy: 0.9481\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.4217 - categorical_accuracy: 0.8768\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.2592 - categorical_accuracy: 0.9028\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 0.2403 - categorical_accuracy: 0.9109\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2715 - categorical_accuracy: 0.9109\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.1946 - categorical_accuracy: 0.9303\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.1126 - categorical_accuracy: 0.9643\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.0869 - categorical_accuracy: 0.9724\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0867 - categorical_accuracy: 0.9789\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0764 - categorical_accuracy: 0.9789\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0638 - categorical_accuracy: 0.9773\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.0415 - categorical_accuracy: 0.9903\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.0429 - categorical_accuracy: 0.9854\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 0.0432 - categorical_accuracy: 0.9935\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0663 - categorical_accuracy: 0.9806\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.0973 - categorical_accuracy: 0.9708\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.0492 - categorical_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train, y_train, epochs=30, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "612dc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b39cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tired'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc33d9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tired'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b946e5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.2762208e-18, 3.2101629e-28, 2.7857067e-15, 3.2203844e-14,\n",
       "       7.1210837e-12, 0.0000000e+00, 1.3792613e-23, 2.3626859e-21,\n",
       "       0.0000000e+00, 7.9935418e-08, 4.9774810e-21, 9.9999988e-01,\n",
       "       0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46d55662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "654ed1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e8691a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68653d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f322805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa21f025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[570,   0],\n",
       "        [  0,  47]],\n",
       "\n",
       "       [[571,   0],\n",
       "        [  0,  46]],\n",
       "\n",
       "       [[570,   0],\n",
       "        [  0,  47]],\n",
       "\n",
       "       [[568,   0],\n",
       "        [  0,  49]],\n",
       "\n",
       "       [[572,   0],\n",
       "        [  0,  45]],\n",
       "\n",
       "       [[567,   0],\n",
       "        [  0,  50]],\n",
       "\n",
       "       [[568,   0],\n",
       "        [  0,  49]],\n",
       "\n",
       "       [[570,   0],\n",
       "        [  0,  47]],\n",
       "\n",
       "       [[567,   0],\n",
       "        [  0,  50]],\n",
       "\n",
       "       [[570,   0],\n",
       "        [  0,  47]],\n",
       "\n",
       "       [[569,   0],\n",
       "        [  0,  48]],\n",
       "\n",
       "       [[574,   0],\n",
       "        [  0,  43]],\n",
       "\n",
       "       [[568,   0],\n",
       "        [  0,  49]]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "070cce6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "804fefbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.85\n",
    "tmp = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "        #sequence.append(keypoints)\n",
    "        sequence.insert(0,keypoints)\n",
    "        sequence = sequence[:30]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            if tmp != np.argmax(res):\n",
    "                tmp = np.argmax(res)\n",
    "                #print(actions[np.argmax(res)])\n",
    "            \n",
    "        if (res[tmp] > threshold).all():\n",
    "            if len(sentence) > 0:\n",
    "                if actions[tmp] != sentence[-1]:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            else:\n",
    "                sentence.append(actions[tmp])\n",
    "        \n",
    "        if len(sentence) > 5:\n",
    "            sentence = sentence[-5:]\n",
    "            \n",
    "        \n",
    "        \n",
    "        cv2.rectangle(image, (0,0),(640,40), (245,117,16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfaea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter-tensorboard\n",
      "  Downloading jupyter_tensorboard-0.2.0.tar.gz (15 kB)\n",
      "Requirement already satisfied: notebook>=5.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jupyter-tensorboard) (6.3.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (4.7.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.0.7)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (0.10.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (0.2.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (0.9.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (20.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.1)\n",
      "Requirement already satisfied: nbformat in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (5.0.5)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.1.12)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (20.0.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (1.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (2.11.3)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\yun\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (5.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jupyter-client>=5.3.4->notebook>=5.0->jupyter-tensorboard) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jupyter-core>=4.6.1->notebook>=5.0->jupyter-tensorboard) (227)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook>=5.0->jupyter-tensorboard) (1.15.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=5.0->jupyter-tensorboard) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=5.0->jupyter-tensorboard) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yun\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.0->jupyter-tensorboard) (2.20)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipykernel->notebook>=5.0->jupyter-tensorboard) (7.22.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.17.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (2.8.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (5.0.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (52.0.0.post20210125)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (3.0.17)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\yun\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jinja2->notebook>=5.0->jupyter-tensorboard) (1.1.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (1.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.5.3)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.7.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.8.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (3.3.0)\n",
      "Requirement already satisfied: async-generator in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.0->jupyter-tensorboard) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.0->jupyter-tensorboard) (1.5.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.0->jupyter-tensorboard) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter-tensorboard) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter-tensorboard) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\yun\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.0->jupyter-tensorboard) (20.9)\n",
      "Requirement already satisfied: webencodings in c:\\users\\yun\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.0->jupyter-tensorboard) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\yun\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=5.0->jupyter-tensorboard) (2.4.7)\n",
      "Building wheels for collected packages: jupyter-tensorboard\n",
      "  Building wheel for jupyter-tensorboard (setup.py): started\n",
      "  Building wheel for jupyter-tensorboard (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter-tensorboard: filename=jupyter_tensorboard-0.2.0-py2.py3-none-any.whl size=15258 sha256=f3a98263de2bf9e8ccf950c09e6bad9ccae748cff333ab991c3b8cc2143e8284\n",
      "  Stored in directory: c:\\users\\yun\\appdata\\local\\pip\\cache\\wheels\\04\\d8\\14\\cd409c6b7d6fd055ec050d65446498357abcec8d81bab11c21\n",
      "Successfully built jupyter-tensorboard\n",
      "Installing collected packages: jupyter-tensorboard\n",
      "Successfully installed jupyter-tensorboard-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter-tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
